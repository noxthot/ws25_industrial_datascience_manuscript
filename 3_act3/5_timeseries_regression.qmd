# Time Series Regression

::: {.callout-caution}
## Still under construction

This section is still under construction and will be completed in the near future.
Please do not go beyond this point for now.
:::

Time series regression belongs to the class of supervised learning and is defined as the task of predicting a continuous target variable based on time series input data.

The [github repository of sktime (accessed: 02 10 25)](https://github.com/sktime/sktime/tree/main/sktime/regression) categorizes its implemented time series regression methods into the following categories:
- **Distance-based**: Methods that use distance measures to compare time series (e.g., k-nearest neighbors).
- **Interval-based**: Methods that extract features from intervals of the time series (e.g., Time Series Forest Regression).
- **Deep learning-based**: Methods that use deep learning architectures.
- **Kernel-based**: Methods that use kernel functions to measure similarity between time series (e.g. Time Series Support Vector Regression).
- **Composed methods**: Methods that combine multiple models.

Again we observe that these categories can directly be mapped to those in @{sec-act3-cluster-algorithms} and @{sec-act3-classification}.

An easy baseline method for time series regression is to use a $k$-nearest neighbors regressor with a suitable distance measure for time series, e.g. dynamic time warping (DTW).
In contrast to nearest neighbor in classification, where the predicted label is determined by e.g. a majority vote of the nearest neighbors, in nearest neighbor regression the predicted value is typically computed as the average (or weighted average) of the target values of the nearest neighbors.
This is implemented in `sktime` as [`KNeighborsTimeSeriesRegressor`](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.regression.distance_based.KNeighborsTimeSeriesRegressor.html).

Also tree-based methods like Random Forests can be adapted for time series regression by simply taking e.g. the mean value of the target variable of the samples in a leaf node as the predicted value.
This is implemented in `sktime` as [`TimeSeriesForestRegressor`](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.regression.interval_based.TimeSeriesForestRegressor.html).
Note that this is to date (02 10 2025) not a very parametrizable implementation; the only tunable parameter is the number of estimators (trees) and the minimum width of the intervals.

In this section we will have a brief look at a deep-learning based method in the context of time series regression, namely a convolutional neural network (CNN) based regressor.

### Convolutional Neural Network (CNN) Regressor

::: {.callout-note}
Due to a lack of suitable open datasets for time series regression, there will not be an example notebook for this section.
In industry behind closed doors however, you will often find regression problems where the input data is time series (e.g. predicting end of life of machine parts based on sensor data).
:::

CNNs, by design, are able to capture local patterns in time series data through the use of convolutional layers.
These layers apply filters that slide over the input data, allowing the model to learn local temporal patterns that are important.
At the same time, CNNs can also capture global patterns by stacking multiple convolutional layers and using pooling operations.
This hierarchical structure enables the model to learn both local and global features of the time series data, making CNNs versatile for various time series tasks.

@zhao2017 propose a CNN architecture specifically designed for time series classification.
The architecture consists of a series of 1D convolutional layers followed by a pooling layer.
The convolutional layers are responsible for extracting local features from the time series data, while the pooling layer helps to reduce the dimensionality and capture more global patterns.
After the convolutional and pooling layers, the model includes a fully connected layer (the authors refer to it as a _feature layer_) that maps the extracted features to $n$ output nodes, where $n$ is the number of classes.
The node with the highest value determines the predicted class.

For regression, `sktime` provides [`CNNRegressor`](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.regression.deep_learning.cnn.CNNRegressor.html), which is based on the architecture described in @{zhao2017}.
To adapt the architecture for regression tasks, the final layer is modified to have a single output node with a linear activation function that predicts a continuous value instead of class probabilities.

::: {.callout-caution appearance="simple" collapse="true" icon=false}
Currently (`sktime` version 0.39.0), `CNNRegressor` unfortunately is not only changing the activation function of the last layer, but also of every other layer to linear.
This effectively hinders the model's ability to learn complex patterns in the data, as linear activations do not introduce non-linearity into the model.
I have raised an [issue](https://github.com/sktime/sktime/issues/8878) to address this.

To avoid this bug, either perform a regression where the target variable is non-negative, allowing you to use the `relu` activation function in the output layer, which is also a good choice for the hidden layers.

Or simply build your own CNN using `pytorch`.
:::
